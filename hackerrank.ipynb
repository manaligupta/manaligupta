{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNotNVueBSfjIiZDYr6tQMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manaligupta/manaligupta/blob/main/hackerrank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOCytfOw8mYO",
        "outputId": "6ee8740d-437a-49fc-e077-f366cd64b9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.0`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npgmEjkU1iJp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import linear_model as lm\n",
        "from sklearn import preprocessing as pp\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F, N = map(int, input().split())\n",
        "train = np.array([input().split() for _ in range(N)], float)\n",
        "print(\"t=\", train)\n",
        "T = int(input())\n",
        "test = np.array([input().split() for _ in range(T)], float)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN3ejqfj688Z",
        "outputId": "0d9624a5-adf9-4d10-8399-b3dcfe14216c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 4\n",
            "3 7 8\n",
            "2 5 7\n",
            "3 9 1\n",
            "4 7 5\n",
            "t= [[3. 7. 8.]\n",
            " [2. 5. 7.]\n",
            " [3. 9. 1.]\n",
            " [4. 7. 5.]]\n",
            "2\n",
            "2 7\n",
            "7 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(type(train))\n",
        "mod = lm.LinearRegression()\n",
        "XtoP = pp.PolynomialFeatures(4, include_bias=False)\n",
        "model = mod.fit(XtoP.fit_transform(train[:, :-1]), train[:, -1])\n",
        "print(\"fit \", XtoP)\n",
        "ymod = mod.predict(XtoP.fit_transform(test))\n",
        "print(\"test\", test)\n",
        "print(*ymod, sep='\\n')"
      ],
      "metadata": {
        "id": "E9_U47nN1nb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e45181-993c-480d-def9-d1ee804eb4ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "fit  PolynomialFeatures(degree=4, include_bias=False)\n",
            "test [[2. 7.]\n",
            " [7. 2.]]\n",
            "4.331382107900849\n",
            "-77.09093729327964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"intercept: {model.intercept_}\")\n",
        "print(f\"slope: {model.coef_}\")\n"
      ],
      "metadata": {
        "id": "19S8tv8SuSxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = np.array([input().split() for _ in range(N)], int)\n"
      ],
      "metadata": {
        "id": "0VcjwF4fFfjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[:, :-3])\n",
        "print(train[:, -2])"
      ],
      "metadata": {
        "id": "Sr-QlRyEBUXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import linear_model as lm\n",
        "from sklearn import preprocessing as pp\n",
        "\n",
        "\n",
        "source = 'https://s3.amazonaws.com/hr-testcases/399/assets/trainingdata.txt'\n",
        "from urllib.request import urlopen\n",
        "data = urlopen(source).read()\n",
        "print(type(data))\n",
        "#table_data = pd.DataFrame(data)\n",
        "#print(\"table=\", table_data)\n",
        "\n",
        "\n",
        "from io import StringIO\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "s=str(data,'utf-8')\n",
        "\n",
        "datas = StringIO(s) \n",
        "\n",
        "df=pd.read_csv(datas, header=None)\n",
        "print(df)\n",
        "x = df[0]\n",
        "x_ = np.array(x)\n",
        "y = df[1]\n",
        "y_ = np.array(y)\n",
        "\n",
        "plt.scatter(x_,y_)\n",
        "plt.xlabel('battery charged')\n",
        "# naming the y axis\n",
        "plt.ylabel('battery lasted')\n",
        "\n",
        "mod = lm.LinearRegression()\n",
        "XtoP = pp.PolynomialFeatures(include_bias=False)\n",
        "model = mod.fit(XtoP.fit_transform(x_.reshape(-1,1)), y)\n",
        "#print(\"fit \", XtoP)\n",
        "timeCharged = float(input().strip())\n",
        "\n",
        "ymod = mod.predict(XtoP.fit_transform(np.array(timeCharged).reshape(-1,1)))\n",
        "print(*ymod, sep='\\n')"
      ],
      "metadata": {
        "id": "wvsxX9osVCit",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "e4b54ac9-f3e5-46ce-f3b5-071ad9e73857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'bytes'>\n",
            "       0     1\n",
            "0   2.81  5.62\n",
            "1   7.14  8.00\n",
            "2   2.72  5.44\n",
            "3   3.87  7.74\n",
            "4   1.90  3.80\n",
            "..   ...   ...\n",
            "95  4.38  8.00\n",
            "96  8.06  8.00\n",
            "97  8.05  8.00\n",
            "98  1.10  2.20\n",
            "99  6.65  8.00\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "0.09\n",
            "0.3187798447820879\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#laptop code in hackerrank\n",
        "\n",
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import linear_model as lm\n",
        "from sklearn import preprocessing as pp\n",
        "\n",
        "source = 'trainingdata.txt'\n",
        "#from urllib.request import urlopen\n",
        "data = open(source).readlines()\n",
        "#print(\"data \", data)\n",
        "x= []\n",
        "y= []\n",
        "\n",
        "#list1= data.split(\"\\n\")\n",
        "#print(\"list \", list1)\n",
        "for line in data:\n",
        "    #print(\"line \", line)\n",
        "    a, b = line.split(\",\")\n",
        "    x.append(float(a))\n",
        "    y.append(float(b))\n",
        "\n",
        "#print(\"x= \", x, \"y = \", y)\n",
        "\n",
        "\n",
        "x_ = np.array(x)\n",
        "y_ = np.array(y)\n",
        "\n",
        "mod = lm.LinearRegression()\n",
        "XtoP = pp.PolynomialFeatures(2, include_bias=False)\n",
        "model = mod.fit(XtoP.fit_transform(x_.reshape(-1,1)), y)\n",
        "#print(\"fit \", XtoP)\n",
        "timeCharged = float(input().strip())\n",
        "\n",
        "ymod = mod.predict(XtoP.fit_transform(np.array(timeCharged).reshape(-1,1)))\n",
        "print(*ymod, sep='\\n')\n"
      ],
      "metadata": {
        "id": "B88sJNKfqMHk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "68cc46e9-0685-41bc-b3c3-74e56690ee34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-2708a560e775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'trainingdata.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#from urllib.request import urlopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#print(\"data \", data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trainingdata.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VXgyYtyQt8o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"     My name is Manali.\\n I am 32 yrs old women.   \"\n",
        "print(s)\n",
        "\n",
        "list1 = s.strip().split(\" \")\n",
        "print(\"list1 \", list1)\n",
        "\n",
        "arr = np.array(list1)\n",
        "arr = arr.reshape(-1,2)\n",
        "print(\"arr \", arr)"
      ],
      "metadata": {
        "id": "voiBkr1t9fO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"My name is %s. I am %d yrs old\" % (\"Manali\", 32))"
      ],
      "metadata": {
        "id": "NhvlS0eE9NsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here. Read input from STDIN. Print output to STDOUT\n",
        "#import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "Physics = [15, 12, 8, 8, 7, 7, 7, 6, 5, 3]\n",
        "History = [10, 25, 17, 11, 13, 17, 20, 13, 9, 15]\n",
        "corr, _ = pearsonr(Physics, History)\n",
        "print(corr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDhb-MGch-_R",
        "outputId": "a1cf014d-6b57-49f4-ae38-6160df830b61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1449981545806852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Confidence interval\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "N = int(input())\n",
        "a = input()\n",
        "#print(type(a))\n",
        "list_a = a.split(\" \")  \n",
        "#print(list_a)\n",
        "list_a1 = []\n",
        "for i in range(len(list_a)):\n",
        "    list_a1.append(int(list_a[i]))\n",
        "#print(list_a1)\n",
        "ci = st.t.interval(alpha=0.90, df=len(list_a1)-1, loc=np.mean(list_a1), scale=st.sem(list_a1))\n",
        "print(ci)"
      ],
      "metadata": {
        "id": "mTQkYhVto5sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics as st\n",
        "print(st.mode([\"few\", \"few\", \"many\", \"some\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "cA9iUnxclLvI",
        "outputId": "9a1676bf-061c-4f0e-a20e-850b7c54e67f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "StatisticsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mStatisticsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7dcd6492a4fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"few\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"few\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"many\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"some\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"many\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.7/statistics.py\u001b[0m in \u001b[0;36mmode\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         raise StatisticsError(\n\u001b[0;32m--> 506\u001b[0;31m                 \u001b[0;34m'no unique mode; found %d equally common values'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m                 )\n\u001b[1;32m    508\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStatisticsError\u001b[0m: no unique mode; found 2 equally common values"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here. Read input from STDIN. Print output to STDOUT\n",
        "# Confidence interval\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import statistics as stat\n",
        "N = int(input())\n",
        "a = input()\n",
        "#print(type(a))\n",
        "list_a = a.split(\" \")  \n",
        "#print(list_a)\n",
        "list_a1 = []\n",
        "for i in range(len(list_a)):\n",
        "    list_a1.append(int(list_a[i]))\n",
        "#print(list_a1)\n",
        "\n",
        "#mean\n",
        "print(stat.mean(list_a1))\n",
        "\n",
        "#median\n",
        "print(stat.median(list_a1))\n",
        "\n",
        "#mode\n",
        "print(min(stat.multimode(list_a1)))\n",
        "\n",
        "#SD\n",
        "print(stat.pstdev(list_a1))\n",
        "\n",
        "#CI\n",
        "#ci = st.t.interval(alpha=0.95, df=len(list_a1)-1, loc=np.mean(list_a1), scale=st.sem(list_a1))\n",
        "ci = st.norm.interval(alpha=0.95, loc=np.mean(list_a1), scale=st.sem(list_a1))\n",
        "print(ci[0], ci[1])"
      ],
      "metadata": {
        "id": "tb78JDWfoFLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here. Read input from STDIN. Print output to STDOUT\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "source = 'trainingdata.txt'\n",
        "data = open(source).readlines()\n",
        "#print(data)\n",
        "N = data[0]\n",
        "#print(\"N \", N)\n",
        "data.remove(data[0])\n",
        "x = []\n",
        "corpus = []\n",
        "for line in data:\n",
        "    x.append(int(line[0]))\n",
        "    corpus.append(line.split(\" \", 1)[1].strip())\n",
        "#print(\"x \", x)\n",
        "#print(\"y \", y)\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=8000, stop_words='english')\n",
        "X_train = vectorizer.fit_transform(corpus) #return document term matrix\n",
        "model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
        "model.fit(vectorizer.transform(corpus),x)\n",
        "T = int(input())\n",
        "test = []\n",
        "for i in range(T):\n",
        "    inp = input() \n",
        "    test.append(inp)\n",
        "    #print(test)\n",
        "\n",
        "predict = model.predict(vectorizer.transform(test))\n",
        "#print(predict[0])\n",
        "for i in range(len(predict)):\n",
        "    print(predict[i])"
      ],
      "metadata": {
        "id": "4tgO-vEeT83f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document classififcation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "source = 'trainingdata.txt'\n",
        "data = open(source).readlines()\n",
        "#print(data)\n",
        "N = data[0]\n",
        "#print(\"N \", N)\n",
        "data.remove(data[0])\n",
        "x = []\n",
        "corpus = []\n",
        "for line in data:\n",
        "    x.append(int(line[0]))\n",
        "    corpus.append(line.split(\" \", 1)[1].strip())\n",
        "#print(\"x \", x)\n",
        "#print(\"y \", y)\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=8000, stop_words='english')\n",
        "X_train = vectorizer.fit_transform(corpus) #return document term matrix\n",
        "#model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
        "model = svm.LinearSVC()\n",
        "model.fit(vectorizer.transform(corpus),x)\n",
        "T = int(input())\n",
        "test = []\n",
        "for i in range(T):\n",
        "    inp = input() \n",
        "    test.append(inp)\n",
        "    #print(test)\n",
        "#vectorizer.fit_transform(test)\n",
        "predict = model.predict(vectorizer.transform(test))\n",
        "#print(predict[0])\n",
        "for i in range(len(predict)):\n",
        "    print(predict[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "yxzVoz1UM94d",
        "outputId": "001db109-a0ea-41d7-d667-ca4f72a7cfb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-769ba7ef3b9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'trainingdata.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#print(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trainingdata.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here. Read input from STDIN. Print output to STDOUT\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.impute import SimpleImputer # used for handling missing data\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # used for encoding categorical data\n",
        "from sklearn.preprocessing import StandardScaler # used for feature scaling\n",
        "import numpy as np\n",
        "source = 'trainingdata.txt'\n",
        "data = open(source).readlines()\n",
        "#print(data)\n",
        "N = data[0]\n",
        "data.remove(data[0])\n",
        "x = []\n",
        "corpus = []\n",
        "for line in data:\n",
        "    x.append(int(line[0]))\n",
        "    corpus.append(line.split(\" \", 1)[1].strip())\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=8000, stop_words='english')\n",
        "X_train = vectorizer.fit_transform(corpus) #return document term matrix\n",
        "#model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
        "print(X_train)\n",
        "\n",
        "labelencoder_X = LabelEncoder()\n",
        "corpus = labelencoder_X.fit_transform(corpus)\n",
        "onehotencoder = OneHotEncoder()\n",
        "X_train = onehotencoder.fit_transform(corpus.reshape(-1,1)).toarray()\n",
        "labelencoder_Y = LabelEncoder()\n",
        "x = labelencoder_Y.fit_transform(x)\n",
        "\n",
        "model = svm.LinearSVC(class_weight = 'balanced')\n",
        "model.fit(corpus.reshape(-1,1),x)\n",
        "T = int(input())\n",
        "test = []\n",
        "for i in range(T):\n",
        "    inp = input() \n",
        "    test.append(inp)\n",
        "#vectorizer.fit_transform(test)\n",
        "predict = model.predict(test)\n",
        "for i in range(len(predict)):\n",
        "    print(predict[i])\n"
      ],
      "metadata": {
        "id": "w4-ByDLbgTs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document classification\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import svm\n",
        "from sklearn.impute import SimpleImputer # used for handling missing data\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # used for encoding categorical data\n",
        "from sklearn.preprocessing import StandardScaler # used for feature scaling\n",
        "import numpy as np\n",
        "source = 'trainingdata.txt'\n",
        "data = open(source).readlines()\n",
        "#print(data)\n",
        "N = data[0]\n",
        "data.remove(data[0])\n",
        "x = []\n",
        "corpus = []\n",
        "for line in data:\n",
        "    x.append(int(line[0]))\n",
        "    corpus.append(line.split(\" \", 1)[1].strip())\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=8000, stop_words='english')\n",
        "X_train = vectorizer.fit_transform(corpus) #return document term matrix\n",
        "#model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
        "#print(\"X train \", X_train)\n",
        "\n",
        "labelencoder_X = LabelEncoder()\n",
        "corpus = labelencoder_X.fit_transform(corpus)\n",
        "onehotencoder = OneHotEncoder()\n",
        "X_train = onehotencoder.fit_transform(corpus.reshape(-1,1)).toarray()\n",
        "#for i in X_train[100]:\n",
        "#    if i != 0:\n",
        "        #print(\"X train1 \", i)\n",
        "labelencoder_Y = LabelEncoder()\n",
        "x = labelencoder_Y.fit_transform(x)\n",
        "\n",
        "model = svm.SVC(class_weight = 'balanced')\n",
        "model.fit(corpus.reshape(-1,1),x)\n",
        "T = int(input())\n",
        "test = []\n",
        "for i in range(T):\n",
        "    inp = input() \n",
        "    test.append(inp)\n",
        "vectorizer.fit_transform(test)\n",
        "\n",
        "labelencoder_X = LabelEncoder()\n",
        "test = labelencoder_X.fit_transform(test)\n",
        "onehotencoder = OneHotEncoder()\n",
        "X_train = onehotencoder.fit_transform(test.reshape(-1,1)).toarray()\n",
        "\n",
        "predict = model.predict(test.reshape(-1,1))\n",
        "\n",
        "for i in predict:\n",
        "    print(i)\n",
        "\n"
      ],
      "metadata": {
        "id": "RGtsgnrVCSnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best aptitude\n",
        "from scipy.stats import pearsonr\n",
        "T = int(input())\n",
        "N = int(input())\n",
        "list1 = []\n",
        "\n",
        "for i in range(6):\n",
        "    list3 = []\n",
        "    str = input()\n",
        "    list2 = str.strip().split(\" \")\n",
        "    for x in list2:\n",
        "        list3.append(float(x))\n",
        "    #print(list3)\n",
        "    list1.append(list3)\n",
        "#print(list1)\n",
        "list4 = []\n",
        "for i in range(5):\n",
        "    corr, _ = pearsonr(list1[0], list1[i+1])\n",
        "    list4.append(corr)\n",
        "max = -100000\n",
        "for i in range(len(list4)):\n",
        "    if max< list4[i]:\n",
        "        max = list4[i]\n",
        "        index = i\n",
        "print(index+1)"
      ],
      "metadata": {
        "id": "Xf33W1xThXq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here. Read input from STDIN. Print output to STDOUT\n",
        "# Stack Exchange Question classifier\n",
        "    \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn import svm\n",
        "\n",
        "def convertTuple(tup):\n",
        "    str = ' '.join(tup)\n",
        "    return str\n",
        "\n",
        "data =open(\"training.json\").readlines()\n",
        "#print(type(data))\n",
        "topic =[]\n",
        "questions = []\n",
        "excerpt = []\n",
        "\n",
        "for s in data[1:]:\n",
        "    s = s.replace('\"', '')\n",
        "    s = s.replace('{', '')\n",
        "    s = s.replace('}', '')\n",
        "    s = s.replace('topic:', '')\n",
        "    s = s.replace('question:', '')\n",
        "    s = s.replace('excerpt:', '')\n",
        "    list1 = s.strip().split(\",\")\n",
        "    tuple1 = tuple(list1)\n",
        "    #print(tuple1)\n",
        "    topic.append(tuple1[0])\n",
        "    questions.append(tuple1[1])\n",
        "    str = convertTuple(tuple1[2:])\n",
        "    #print(str)\n",
        "    excerpt.append(str)\n",
        "    \n",
        "tuple2 = {'topics': topic, 'questions': questions, 'excerpt': excerpt}\n",
        "  \n",
        "df = pd.DataFrame(tuple2)\n",
        "#print(df.head())\n",
        "#print(df.iloc[2])\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=8000, stop_words='english')\n",
        "X_train = vectorizer.fit_transform(excerpt) #return document term matrix\n",
        "#model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
        "model = svm.LinearSVC()\n",
        "model.fit(vectorizer.transform(excerpt),topic)\n",
        "T = int(input())\n",
        "test = []\n",
        "for i in range(T):\n",
        "    inp = input() \n",
        "    test.append(inp)\n",
        "    #print(test)\n",
        "#vectorizer.fit_transform(test)\n",
        "predict = model.predict(vectorizer.transform(test))\n",
        "#print(predict[0])\n",
        "for i in range(len(predict)):\n",
        "    print(predict[i])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DEKxkYo8Mkch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here. Read input from STDIN. Print output to STDOUT\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "str = input()\n",
        "lines = str.split(\".\")\n",
        "question = []\n",
        "for i in range(5):\n",
        "    question.append(input())\n",
        "    \n",
        "print(\"str\", lines)\n",
        "print(\"questions \", question)\n",
        "tfidf_ = []\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=8000,stop_words='english')\n",
        "X_train = vectorizer.fit_transform(lines)\n",
        "print(X_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "847ClLQ7gha9",
        "outputId": "6e69a7ce-9b97-4c3c-e779-33aed642cfb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zebras are several species of African equids (horse family) united by their distinctive black and white stripes. Their stripes come in different patterns, unique to each individual. They are generally social animals that live in small harems to large herds. Unlike their closest relatives, horses and donkeys, zebras have never been truly domesticated. There are three species of zebras: the plains zebra, the Grévy's zebra and the mountain zebra. The plains zebra and the mountain zebra belong to the subgenus Hippotigris, but Grévy's zebra is the sole species of subgenus Dolichohippus. The latter resembles an ass, to which it is closely related, while the former two are more horse-like. All three belong to the genus Equus, along with other living equids. The unique stripes of zebras make them one of the animals most familiar to people. They occur in a variety of habitats, such as grasslands, savannas, woodlands, thorny scrublands, mountains, and coastal hills. However, various anthropogenic factors have had a severe impact on zebra populations, in particular hunting for skins and habitat destruction. Grévy's zebra and the mountain zebra are endangered. While plains zebras are much more plentiful, one subspecies, the quagga, became extinct in the late 19th century – though there is currently a plan, called the Quagga Project, that aims to breed zebras that are phenotypically similar to the quagga in a process called breeding back.\n",
            "Which Zebras are endangered?\n",
            "What is the aim of the Quagga Project?\n",
            "Which animals are some of their closest relatives?\n",
            "Which are the three species of zebras?\n",
            "Which subgenus do the plains zebra and the mountain zebra belong to?\n",
            "str ['Zebras are several species of African equids (horse family) united by their distinctive black and white stripes', ' Their stripes come in different patterns, unique to each individual', ' They are generally social animals that live in small harems to large herds', ' Unlike their closest relatives, horses and donkeys, zebras have never been truly domesticated', \" There are three species of zebras: the plains zebra, the Grévy's zebra and the mountain zebra\", \" The plains zebra and the mountain zebra belong to the subgenus Hippotigris, but Grévy's zebra is the sole species of subgenus Dolichohippus\", ' The latter resembles an ass, to which it is closely related, while the former two are more horse-like', ' All three belong to the genus Equus, along with other living equids', ' The unique stripes of zebras make them one of the animals most familiar to people', ' They occur in a variety of habitats, such as grasslands, savannas, woodlands, thorny scrublands, mountains, and coastal hills', ' However, various anthropogenic factors have had a severe impact on zebra populations, in particular hunting for skins and habitat destruction', \" Grévy's zebra and the mountain zebra are endangered\", ' While plains zebras are much more plentiful, one subspecies, the quagga, became extinct in the late 19th century – though there is currently a plan, called the Quagga Project, that aims to breed zebras that are phenotypically similar to the quagga in a process called breeding back', '']\n",
            "questions  ['Which Zebras are endangered?', 'What is the aim of the Quagga Project?', 'Which animals are some of their closest relatives?', 'Which are the three species of zebras?', 'Which subgenus do the plains zebra and the mountain zebra belong to?']\n",
            "  (0, 177)\t0.22888445084742612\n",
            "  (0, 17)\t0.22888445084742612\n",
            "  (0, 40)\t0.22888445084742612\n",
            "  (0, 169)\t0.22888445084742612\n",
            "  (0, 57)\t0.22888445084742612\n",
            "  (0, 77)\t0.22888445084742612\n",
            "  (0, 47)\t0.22888445084742612\n",
            "  (0, 3)\t0.22888445084742612\n",
            "  (0, 150)\t0.22888445084742612\n",
            "  (0, 192)\t0.22888445084742612\n",
            "  (0, 153)\t0.17626232313786896\n",
            "  (0, 176)\t0.22888445084742612\n",
            "  (0, 16)\t0.22888445084742612\n",
            "  (0, 39)\t0.22888445084742612\n",
            "  (0, 168)\t0.22888445084742612\n",
            "  (0, 56)\t0.22888445084742612\n",
            "  (0, 76)\t0.19810247942917553\n",
            "  (0, 46)\t0.19810247942917553\n",
            "  (0, 2)\t0.22888445084742612\n",
            "  (0, 149)\t0.17626232313786896\n",
            "  (0, 187)\t0.14548035171961832\n",
            "  (1, 166)\t0.31095278794899556\n",
            "  (1, 106)\t0.31095278794899556\n",
            "  (1, 38)\t0.31095278794899556\n",
            "  (1, 33)\t0.31095278794899556\n",
            "  :\t:\n",
            "  (12, 51)\t0.13839717137199417\n",
            "  (12, 124)\t0.13839717137199417\n",
            "  (12, 160)\t0.13839717137199417\n",
            "  (12, 116)\t0.13839717137199417\n",
            "  (12, 191)\t0.13839717137199417\n",
            "  (12, 112)\t0.13839717137199417\n",
            "  (12, 20)\t0.13839717137199417\n",
            "  (12, 119)\t0.13839717137199417\n",
            "  (12, 139)\t0.13839717137199417\n",
            "  (12, 108)\t0.13839717137199417\n",
            "  (12, 18)\t0.13839717137199417\n",
            "  (12, 4)\t0.13839717137199417\n",
            "  (12, 121)\t0.13839717137199417\n",
            "  (12, 21)\t0.27679434274398834\n",
            "  (12, 113)\t0.13839717137199417\n",
            "  (12, 34)\t0.13839717137199417\n",
            "  (12, 24)\t0.13839717137199417\n",
            "  (12, 0)\t0.13839717137199417\n",
            "  (12, 88)\t0.13839717137199417\n",
            "  (12, 50)\t0.13839717137199417\n",
            "  (12, 123)\t0.4151915141159825\n",
            "  (12, 159)\t0.13839717137199417\n",
            "  (12, 115)\t0.13839717137199417\n",
            "  (12, 110)\t0.10657869877757047\n",
            "  (12, 187)\t0.17593217095921757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here. Read input from STDIN. Print output to STDOUT\n",
        "# Matching question answer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "\n",
        "str = input()\n",
        "lines = str.strip().split(\".\")\n",
        "question = []\n",
        "for i in range(5):\n",
        "    question.append(input())\n",
        "ans = input().strip().split(\";\")\n",
        "#print(\"ans\", ans)\n",
        "tfidf_ = []\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=8000,stop_words='english')\n",
        "T_train= vectorizer.fit_transform(lines).toarray()\n",
        "T = vectorizer.transform(lines).toarray()\n",
        "Q = vectorizer.transform(question).toarray()\n",
        "A = vectorizer.transform(ans).toarray()\n",
        "#for i in range(len(lines)):\n",
        "#    print(\"i=\", i)\n",
        "#    print(lines[i])\n",
        "q_t = []\n",
        "for i in range(5):\n",
        "    cos = []\n",
        "    for j in range(len(lines)-1):\n",
        "        cosine = np.dot(Q[i],T[j])/(norm(Q[i])*norm(T[j]))\n",
        "        cos.append(cosine)\n",
        "    #print(cos)\n",
        "    q_t.append(cos.index(max(cos)))\n",
        "#print(\"--\", q_t)\n",
        "for x in q_t:\n",
        "    cos = []\n",
        "    for j in range(len(ans)):\n",
        "        cosine = np.dot(T[x],A[j])/(norm(T[x])*norm(A[j]))\n",
        "        cos.append(cosine)\n",
        "    print(ans[cos.index(max(cos))])\n",
        "    #ans.remove(ans[cos.index(max(cos))]) giving better answer \"i dont know how\"\n"
      ],
      "metadata": {
        "id": "UqFYO9i9girO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quora Answer Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import numpy as np\n",
        "\n",
        "N, M = input().strip().split(\" \")\n",
        "y = []\n",
        "x = []\n",
        "for i in range(int(N)):\n",
        "    list1 = input().strip().split(\":\")\n",
        "    #if i< 5:\n",
        "        #print(list1[0])\n",
        "    if \"+\" in list1[0]:\n",
        "        y.append(\"+1\")\n",
        "    else:\n",
        "        y.append(\"-1\")\n",
        "    x1 = []\n",
        "    for x2 in list1[1:]:\n",
        "        #print(\"x2 \", float(x2.strip().split(\" \")[0]))\n",
        "        x1.append(float(x2.strip().split(\" \")[0]))\n",
        "    x.append(x1)\n",
        "        \n",
        "#print(\"y \",y[0:10])\n",
        "#print(\"x \", x)\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(np.array(x), np.array(y))\n",
        "T = int(input())\n",
        "test = []\n",
        "list2 = []\n",
        "name = []\n",
        "for i in range(T):\n",
        "    list2 = input().strip().split(\":\")\n",
        "    name.append(list2[0].strip().split(\" \")[0])\n",
        "    inp = []\n",
        "    for x2 in list2[1:]:\n",
        "        #print(\"x2 \", float(x2.strip().split(\" \")[0]))\n",
        "        inp.append(float(x2.strip().split(\" \")[0]))\n",
        "    test.append(inp)\n",
        "#print(\"test \", test[0:5])\n",
        "#vectorizer.fit_transform(test)\n",
        "predict = model.predict(np.array(test))\n",
        "#print(\"p \", len(predict))\n",
        "#print(\"n \", len(name))\n",
        "for i in range(len(predict)):\n",
        "    print(name[i]+\" \"+predict[i])\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "eT2EQoeoVFEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict missing grades\n",
        "#from sklearn import svm\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "lines = open('training.json').readlines()\n",
        "#print(lines[1:5])\n",
        "x_train = []\n",
        "y_train = []\n",
        "for line in lines[1:]:\n",
        "    r = line.rstrip(\"\\n\").strip(\"{\").strip(\"}\").split(\",\")\n",
        "    list1 = []\n",
        "    for ri in r[0:-1]: \n",
        "        list1.append(int(ri.split(\":\")[1]))\n",
        "    x_train.append(list1[0:-1])\n",
        "    y_train.append(str(list1[4]))\n",
        "#model = svm.SVC() # Linear Kernel\n",
        "model = KNeighborsClassifier(n_neighbors=7)\n",
        "\n",
        "model.fit(x_train, y_train)\n",
        "T  = int(input())\n",
        "test = []\n",
        "for i in range(T):\n",
        "    r = input().strip(\"{\").strip(\"}\").split(\",\")\n",
        "    #print(\"r \", r)\n",
        "    list1 = []\n",
        "    for ri in r[0:-1]: \n",
        "        list1.append(int(ri.split(\":\")[1]))\n",
        "    test.append(list1)\n",
        "predict = model.predict(test)\n",
        "for i in range(len(predict)):\n",
        "    print(predict[i])\n",
        "    #print(1)\n"
      ],
      "metadata": {
        "id": "W5bRPTSp5lxK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}